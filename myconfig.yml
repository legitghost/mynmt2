model_dir: training

data:
  train_features_file: data/train-multi.src.tok
  train_labels_file: data/train-multi.src.tok
  eval_features_file: data/valid-multi.src.tok
  eval_labels_file: data/valid-multi.tgt.tok
  source_words_vocabulary: data/src-vocab.txt
  target_words_vocabulary: data/tgt-vocab.txt
  
params:
  optimizer: AdamOptimizer
  learning_rate: 0.0002
  clip_gradients: 5.0
  decay_type: exponential_decay
  decay_rate: 0.7
  decay_steps: 100000
  start_decay_steps: 500000
  param_init: 0.1
  clip_gradients: 5.0
  beam_width: 3
  maximum_iterations: 250

train:
  batch_size: 100
  bucket_width: 1
  save_checkpoints_steps: 1000
  save_summary_steps: 50
  train_steps: 200000
  maximum_features_length: 50
  maximum_labels_length: 50
  sample_buffer_size: 10000
  num_gpus: 1

eval:
  eval_delay: 90000 # 25 hours

infer:
  batch_size: 30